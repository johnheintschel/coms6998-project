{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODOs:\n",
    "\n",
    "# Benchmarks: \n",
    "## - Buy and hold market index (Dow Jones)\n",
    "## - Constantly Rebalanced Portfolio (maintain equal weights of each asset at all times)\n",
    "\n",
    "# Non-fractional shares\n",
    "\n",
    "# Contexts\n",
    "\n",
    "# Other trading strategies\n",
    "## Momentum (should be just inverse)\n",
    "## Fundamentals-based strategies (need to get more data for this)\n",
    "\n",
    "# PNL reporting/collecting relevant statistics for visualizations from our simulations\n",
    "\n",
    "# Longer time horizons. Only weight according to n most recent time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## This class contains the components necessary for running and evaluating a historical simulation.\n",
    "\n",
    "class SimulationEnv(object):\n",
    "    \n",
    "    def __init__(self, wealth, assets, \n",
    "                 path_to_data, start_date, end_date,\n",
    "                 agent_type, expert_type):\n",
    "\n",
    "        self.init_wealth = wealth\n",
    "        self.wealth = wealth\n",
    "        self.assets = assets\n",
    "        self.agent_type = agent_type\n",
    "        self.expert_type = expert_type\n",
    "        self.path_to_data = path_to_data\n",
    "        ## TODO: \n",
    "        ## - specify start and end date for simulation\n",
    "        ## - collect statistics on simulation results\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    ## To be called before running a simulation - initialize experts, agents, and initial position, etc.\n",
    "\n",
    "    def setup_params(self, agent_args={}, expert_args={}):\n",
    "    \n",
    "        self.experts = [self.expert_type(a, self.path_to_data, **expert_args) for a in self.assets]\n",
    "        self.agent = self.agent_type(self.experts, **agent_args)\n",
    "        self.positions = [weight * self.wealth for weight in self.agent.weights]\n",
    "        \n",
    "        ## TODO: \n",
    "        # do various other stuff here like for select for high-volatility stocks or something\n",
    "        # exception handling\n",
    "        # Need to make sure data files are in sync\n",
    "        \n",
    "        return\n",
    "        \n",
    "    ## Run simulation\n",
    "    def run(self):\n",
    "        ## Warmup period: \n",
    "        ## i.e. for strategies involving moving average indicators, wait until we have enough data to calculate MA\n",
    "        for expert in self.agent.experts:\n",
    "            expert.warmup()\n",
    "        \n",
    "        ## Simlation: Go until data iterators reach the end\n",
    "        while True:\n",
    "            try:\n",
    "                ## Update experts with last period's rewards\n",
    "                for expert in self.agent.experts:\n",
    "                    expert.update()\n",
    "                ## Update agent accordingly (i.e. for Hedge, update weights according to each expert's reward in the last period)\n",
    "                self.agent.update()\n",
    "                \n",
    "                ## Update position with returns from last period\n",
    "                self.positions = self.positions * (1 + self.agent.returns)\n",
    "                self.wealth = np.sum(self.positions)\n",
    "                \n",
    "                ## Rebalance according to new, updated weights\n",
    "                ## TODO: only allow non-fractional shares to be purchased (?)\n",
    "                self.positions = [weight * self.wealth for weight in self.agent.weights]\n",
    "                \n",
    "            except StopIteration:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Base classes for agents and experts to be implemented by us\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class Agent(object):\n",
    "    __metaclass__ =ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def act(self):\n",
    "        pass\n",
    "    \n",
    "class Expert(object):\n",
    "    __metaclass__=ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, reward_data):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "class Context(object):\n",
    "    __metaclass__=ABCMeta\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, context_data):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def observe(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "## Hedge. (http://www.cis.upenn.edu/~mkearns/finread/helmbold98line.pdf)\n",
    "\n",
    "class Hedge(Agent):\n",
    "    \n",
    "    ## Initialize with a set of experts\n",
    "    def __init__(self, experts, eta):\n",
    "        self.eta = eta\n",
    "        self.experts = experts\n",
    "        self.weights = np.ones(len(self.experts))/len(self.experts)\n",
    "        self.rewards = None\n",
    "        self.returns = None\n",
    "        return\n",
    "    \n",
    "    ## Update the agent's rewards and its weights for each expert\n",
    "    def update(self):\n",
    "        self.rewards = np.asarray([e.reward for e in self.experts])\n",
    "        self.returns = np.asarray([e.returns for e in self.experts])\n",
    "        if (np.sum(self.weights * self.rewards) == 0):\n",
    "            print(\"weights: \", self.weights)\n",
    "            self.weights = (np.ones(len(self.experts))) / (1.0 * len(self.experts))\n",
    "            print(\"reset weights\")\n",
    "            print(\"rewards: \", self.rewards)\n",
    "            print(\"weights: \", self.weights)\n",
    "            print(\"denom\", np.sum(self.weights * self.rewards))\n",
    "        multipliers = np.exp(self.eta * self.rewards/abs(np.sum(self.weights * self.rewards)))\n",
    "        self.weights = (self.weights * multipliers)/ np.sum(self.weights * multipliers)\n",
    "        self.weights = np.nan_to_num(self.weights)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def act(self):\n",
    "        return self.weights\n",
    "\n",
    "## Dummy expert that always pick the same asset\n",
    "class Dummy(Expert):\n",
    "    \n",
    "    ## Expert has a reward associated with its pick\n",
    "    def __init__(self, name, path_to_data):\n",
    "        self.reward = 0.\n",
    "        self.pick = True ## Might not be necessary\n",
    "        self.data = pd.read_csv(path_to_data + name + \".csv\", iterator=True, chunksize=1)\n",
    "        self.last_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "        return\n",
    "    \n",
    "    ## Expert updates its reward \n",
    "    def update(self):\n",
    "        current_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "        self.reward = (current_price - self.last_price)/self.last_price\n",
    "        self.returns = self.reward\n",
    "        self.last_price = current_price\n",
    "        return\n",
    "    \n",
    "    def warmup(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class MeanReversion(Expert):\n",
    "    \n",
    "    def __init__(self, name, path_to_data, window_size, threshold):\n",
    "        self.reward = 0.\n",
    "        self.pick = False\n",
    "        self.data = pd.read_csv(path_to_data + name + \".csv\", iterator=True, chunksize=1)\n",
    "        self.last_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "        self.window_size = window_size\n",
    "        self.avg = 0.0\n",
    "        self.std = 0.0\n",
    "        self.threshold = threshold\n",
    "        self.last_n_prices = Queue(maxsize=10)\n",
    "        self.returns = 0.\n",
    "        return\n",
    "    \n",
    "    def warmup(self):\n",
    "        n = 1\n",
    "        while n <= self.window_size:\n",
    "            self.last_n_prices.put(self.last_price)\n",
    "            self.last_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "            n += 1\n",
    "        return\n",
    "        \n",
    "    def update(self):\n",
    "        _ = self.last_n_prices.get()\n",
    "            \n",
    "        self.last_n_prices.put(self.last_price)\n",
    "        self.avg = np.mean(list(self.last_n_prices.queue))\n",
    "        self.std = np.std(list(self.last_n_prices.queue))\n",
    "        \n",
    "        current_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "\n",
    "        ## If self.pick is True, we bought the stock and our reward is whatever the return was in the last period\n",
    "        if self.pick:\n",
    "            self.reward = (current_price - self.last_price)/self.last_price\n",
    "            self.returns = self.reward\n",
    "        else:\n",
    "            self.reward = -(current_price - self.last_price)/self.last_price\n",
    "            self.returns = 0\n",
    "        self.last_price = current_price\n",
    "\n",
    "        if self.last_price <= self.avg - self.threshold * self.std:\n",
    "            self.pick = True\n",
    "        else:\n",
    "            self.pick = False\n",
    "\n",
    "        return\n",
    "    \n",
    "class Momentum(Expert):\n",
    "    \n",
    "    def __init__(self, name, path_to_data, window_size, threshold):\n",
    "        self.reward = 0.\n",
    "        self.pick = False\n",
    "        self.data = pd.read_csv(path_to_data + name + \".csv\", iterator=True, chunksize=1)\n",
    "        self.last_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "        self.window_size = window_size\n",
    "        self.avg = 0.0\n",
    "        self.std = 0.0\n",
    "        self.threshold = threshold\n",
    "        self.last_n_prices = Queue(maxsize=10)\n",
    "        self.returns = 0.\n",
    "        return\n",
    "    \n",
    "    def warmup(self):\n",
    "        n = 1\n",
    "        while n <= self.window_size:\n",
    "            self.last_n_prices.put(self.last_price)\n",
    "            self.last_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "            n += 1\n",
    "        return\n",
    "        \n",
    "    def update(self):\n",
    "        _ = self.last_n_prices.get()\n",
    "            \n",
    "        self.last_n_prices.put(self.last_price)\n",
    "        self.avg = np.mean(list(self.last_n_prices.queue))\n",
    "        self.std = np.std(list(self.last_n_prices.queue))\n",
    "        \n",
    "        current_price = float(self.data.get_chunk(1)[\"adj_close\"])\n",
    "\n",
    "        ## If self.pick is True, we bought the stock and our reward is whatever the return was in the last period\n",
    "        if self.pick:\n",
    "            self.reward = (current_price - self.last_price)/self.last_price\n",
    "            self.returns = self.reward\n",
    "        else:\n",
    "            self.reward = -(current_price - self.last_price)/self.last_price\n",
    "            self.returns = 0\n",
    "        self.last_price = current_price\n",
    "\n",
    "        if self.last_price >= self.avg - self.threshold * self.std:\n",
    "            self.pick = True\n",
    "        else:\n",
    "            self.pick = False\n",
    "\n",
    "        return\n",
    "    \n",
    "class VolContext(Context):\n",
    "    \n",
    "    def __init__(self, data_file, threshold):\n",
    "        self.data = pd.read_csv(data_file, iterator=True, chunksize=1)\n",
    "        self.threshold = threshold\n",
    "        self.contexts = [\"HighVol\", \"LowVol\"]\n",
    "        return\n",
    "    \n",
    "    # Returns a string giving the name of the context\n",
    "    def observe(self):\n",
    "        if float(self.data.get_chunk(1)[\"adj_close\"]) > self.threshold:\n",
    "            return self.contexts[0]\n",
    "        else:\n",
    "            return self.contexts[1]\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some other examples of agents to use as benchmarks \n",
    "\n",
    "class NaiveBuyHold(Agent):\n",
    "    \n",
    "    def __init__(self, experts):\n",
    "        self.experts = experts\n",
    "        self.weights = np.ones(len(self.experts))/len(self.experts)\n",
    "        self.rewards = None\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        self.rewards = np.asarray([e.reward for e in self.experts])\n",
    "        self.returns = self.rewards\n",
    "        return\n",
    "    \n",
    "    def act(self):\n",
    "        return self.weights\n",
    "    \n",
    "## TODO\n",
    "class WeightedBuyHold(Agent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        return\n",
    "    \n",
    "    def act(self):\n",
    "        return\n",
    "    \n",
    "\n",
    "## TODO\n",
    "class ConstantRebalancer(Agent):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        return\n",
    "    \n",
    "    def act(self):\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE',\n",
    " 'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', \n",
    "'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'TRV', 'UNH', 'UTX', 'V', 'VZ', 'WMT', 'XOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836624.984343\n"
     ]
    }
   ],
   "source": [
    "## Run a simulation with Hedge, each expert is a proxy for buying a given stock\n",
    "\n",
    "s = SimulationEnv(100000, stocks, \"data/djia_20000101_20171101/\", \"2000-01-01\", \"2017-11-01\", Hedge, Dummy)\n",
    "s.setup_params(agent_args = {\"eta\": -0.001})\n",
    "s.run()\n",
    "print (s.wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196477.308454\n"
     ]
    }
   ],
   "source": [
    "## Run simulation where we just buy and hold all stocks in equal amounts\n",
    "\n",
    "s = SimulationEnv(100000, stocks, \"data/djia_20000101_20171101/\", \"2000-01-01\", \"2017-11-01\", NaiveBuyHold, Dummy)\n",
    "s.setup_params(expert_args={})\n",
    "s.run()\n",
    "\n",
    "print (s.wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216633.613026\n"
     ]
    }
   ],
   "source": [
    "s = SimulationEnv(100000, stocks, \"data/djia_20000101_20171101/\", \"2000-01-01\", \"2017-11-01\", Hedge, Momentum)\n",
    "\n",
    "s.setup_params(\n",
    "    agent_args={\"eta\": -0.001},\n",
    "    expert_args={\"window_size\": 10, \"threshold\": 2.0}\n",
    ")\n",
    "\n",
    "s.run()\n",
    "print (s.wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250265.553842\n"
     ]
    }
   ],
   "source": [
    "## Run simulation where we each expert only recommends to buy when the price crosses 0.5 standard deviations below the 10-day moving average. \n",
    "## Otherwise doesn't do anything\n",
    "## Sell position when price gets above 0.5 standard deviations below the 10-day MA\n",
    "\n",
    "s = SimulationEnv(100000, stocks, \"data/djia_20000101_20171101/\", \"2000-01-01\", \"2017-11-01\", Hedge, MeanReversion)\n",
    "\n",
    "s.setup_params(\n",
    "    agent_args={\"eta\": -0.001},\n",
    "    expert_args={\"window_size\": 10, \"threshold\": .5}\n",
    ")\n",
    "\n",
    "s.run()\n",
    "print (s.wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
